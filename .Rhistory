knitr::opts_chunk$set(echo = TRUE)
library(readxl)
library(ggplot2)
library(stats)
library(dplyr)
library(vcd)
library(glmnet)
library(insurancerating)
library(stats)
library(caret)
library(pscl)
library(arules)
library(rsample)
library(MASS)
library(regclass)
data <- read_excel("./data/base_challenge.xlsx") %>%
filter( Veh_val > 0, Edad < 100) %>%
dplyr::select(-C_postal, -C_rcmat_culpa, -C_rcmat_inoc)
data$fpag <- factor(data$fpag, labels = c('Anual efectivo', 'Anual domiciliado',"semestral esfectivo", "semestral 1ºp efectivo", "semestral domiciliado"))
knitr::opts_chunk$set(echo = TRUE)
library(readxl)
library(ggplot2)
library(stats)
library(dplyr)
library(vcd)
library(glmnet)
library(insurancerating)
library(stats)
library(caret)
library(pscl)
library(arules)
library(rsample)
library(MASS)
library(regclass)
data <- read_excel("base_challenge.xlsx") %>%
filter( Veh_val > 0, Edad < 100) %>%
dplyr::select(-C_postal, -C_rcmat_culpa, -C_rcmat_inoc)
knitr::opts_chunk$set(echo = TRUE)
library(readxl)
library(ggplot2)
library(stats)
library(dplyr)
library(vcd)
library(glmnet)
library(insurancerating)
library(stats)
library(caret)
library(pscl)
library(arules)
library(rsample)
library(MASS)
library(regclass)
data <- read_excel("./data/base_challenge.xlsx") %>%
filter( Veh_val > 0, Edad < 100) %>%
dplyr::select(-C_postal, -C_rcmat_culpa, -C_rcmat_inoc)
data$fpag <- factor(data$fpag, labels = c('Anual efectivo', 'Anual domiciliado',"semestral esfectivo", "semestral 1ºp efectivo", "semestral domiciliado"))
#data$C_postal <- factor(data$C_postal)
data$Veh_marca <- factor(data$Veh_marca)
data$Veh_cuso <- factor(data$Veh_cuso)#codificado en dos valores , 3 y 4. Pero no sabemos las etiquetas
data$Veh_comb <- factor(data$Veh_comb)
data$Cus_des <- factor(data$Cus_des)
data$b7_puertas <- factor(data$b7_puertas, ordered = TRUE)
str(data)#controlamos los datos
summary(data)
# Función que discretiza en cuantiles
quantbreaks <- function(predictor,cortes=10,cuantiles=F){
qnt <- quantile(predictor,probs=seq(0,1,1/cortes),include.lowest=T)
qnt[1] <- qnt[1]-0.01
qnt[cortes] <- qnt[cortes]+0.01
if(cuantiles){return(qnt)}                # devuelve cortes
else{return(cut(predictor,breaks=qnt))} # devuelve var discretizada
}
data2 <- data
data2 <- data2 %>% mutate(Edad_Cat = quantbreaks(data$Edad, cortes=10), Expo_Cat =quantbreaks(data$Expo, cortes=10),
Carne_Cat=quantbreaks(data$Carne, cortes=10), Veh_cdin_Cat = quantbreaks(data$Veh_cdin, cortes=5),
Veh_val_Cat=quantbreaks(data$Veh_val, cortes=10))
# Añadimos una columna con suma de reclamaciones
data3 <- data2 %>% dplyr::mutate(N_totales = N_rcmat + N_rccorp)
obs <- data3 %>% group_by(N_rcmat=N_rcmat) %>%
summarise(observados=round(n()/24995,3))
esperados <- round(dpois(0:4,mean(data3$N_rcmat)),3)
cbind(obs,esperados)
siniestros <- data3%>% group_by(N_rcmat)%>% summarise(observados=n()/dim(data3)[1])
siniestros <- siniestros%>% mutate(esperados=dpois(N_rcmat,lambda = mean(data3$N_rcmat)))
siniestros %>% ggplot()+geom_col(aes(x=N_rcmat,y=observados),color="blue",fill="blue",alpha=0.1)+
geom_col(aes(x=N_rcmat,y=esperados),color="red",fill="red",alpha=0.1)
fit.bn <- goodfit(data3$N_rcmat,type = "nbinomial", method="MinChisq")
# goodfit essentially computes the fitted values of a discrete distribution (either Poisson,
# binomial or negative binomial) to the count data given in x. If the parameters are not specified # they are estimated either by ML or Minimum Chi-squared.
negbinData <- data.frame(N_rcmat=0:4,
observados=fit.bn$observed/dim(data3)[1],
esperados=fit.bn$fitted/dim(data3)[1])
negbinData%>% ggplot +
geom_col(aes(x=N_rcmat,y=observados),color="blue",fill="blue",alpha=0.1) +
geom_col(aes(x=N_rcmat,y=esperados),color="red",fill="red",alpha=0.05)
obs <- data3 %>% group_by(N_rccorp=N_rccorp) %>%
summarise(observados=round(n()/24995,3))
esperados <- round(dpois(0:2,mean(data3$N_rccorp)),3)
cbind(obs,esperados)
siniestros <- data3%>% group_by(N_rccorp)%>% summarise(observados=n()/dim(data3)[1])
siniestros <- siniestros%>% mutate(esperados=dpois(N_rccorp,lambda = mean(data3$N_rccorp)))
siniestros %>% ggplot()+geom_col(aes(x=N_rccorp,y=observados),color="blue",fill="blue",alpha=0.1)+
geom_col(aes(x=N_rccorp,y=esperados),color="red",fill="red",alpha=0.1)
obs <- data3 %>% group_by(N_totales=N_totales) %>%
summarise(observados=round(n()/24995,3))
esperados <- round(dpois(0:5,mean(data3$N_totales)),3)
cbind(obs,esperados)
siniestros <- data3%>% group_by(N_totales)%>% summarise(observados=n()/dim(data3)[1])
siniestros <- siniestros%>% mutate(esperados=dpois(N_totales,lambda = mean(data3$N_totales)))
siniestros %>% ggplot()+geom_col(aes(x=N_totales,y=observados),color="blue",fill="blue",alpha=0.1)+
geom_col(aes(x=N_totales,y=esperados),color="red",fill="red",alpha=0.1)
fit.bn <- goodfit(data3$N_totales,type = "nbinomial", method="MinChisq")
# goodfit essentially computes the fitted values of a discrete distribution (either Poisson,
# binomial or negative binomial) to the count data given in x. If the parameters are not specified # they are estimated either by ML or Minimum Chi-squared.
negbinData <- data.frame(N_totales=0:5,
observados=fit.bn$observed/dim(data3)[1],
esperados=fit.bn$fitted/dim(data3)[1])
negbinData%>% ggplot +
geom_col(aes(x=N_totales,y=observados),color="blue",fill="blue",alpha=0.1) +
geom_col(aes(x=N_totales,y=esperados),color="red",fill="red",alpha=0.05)
data3 <- data2
#data4 <- data3 %>% dplyr::select(-poliza,-Edad,-Carne,-fini,-fvto,-Veh_cdin,-Veh_val,-Veh_peso,-Veh_cuso,-N_rcmat,-N_rccorp,-N_culpa,-N_inoc,-C_rcmat_agregado,-C_rccorp,-fpag,-Edad_Cat,-veh_ant,-Cus_des,-b7_puertas,-Veh_val_Cat,-clase_veh,-Tipo_veh-Expo)
data4 <- data3 %>% dplyr::mutate(N_totales = N_rcmat + N_rccorp) %>%
dplyr::select(-poliza,
-Edad,-Carne,
-fini,-fvto,
-Veh_cdin,-Veh_val,
-Veh_peso,-Veh_cuso,-N_rcmat,-N_rccorp,-N_culpa,-N_inoc,-C_rcmat_agregado,-C_rccorp,
-Edad_Cat,
-veh_ant,-Cus_des,-clase_veh,-Tipo_veh)
# discretizamos en teniendo en cuenta la función objetivo
#library(OneR)
#var.num.disc <- optbin(formula=N_totales~Edad+Carne+Veh_cdin+Veh_val, data=data4, method = c("logreg"))
#var.num.disc <- var.num.disc %>% dplyr::select(-N_totales)
#data4[,names(var.num.disc)] <- var.num.disc
# Sustituimos los valores con Expo cero por la media de exposición
mediana_expo <- median(data4$Expo)
data4$Expo[which(data4$Expo==0)] <- mediana_expo
set.seed(1500)
particion <- initial_split(data4,prop = 2/3, strata = N_totales) # particion aleatoria pura
train_data <- training(particion)
test_data <- testing(particion)
# CREAMOS EL MODELO CON TODO Y HACEMOS UN STEP
# usamos una variable de offset que será el logaritmo del tiempo de exposición de la poliza
of_var <- log(train_data$Expo/365.25)
train_data <- train_data %>% dplyr::select(-Expo)
NB.log <- glm.nb(N_totales~.+offset(of_var) ,data=train_data,link="log",control=glm.control(maxit=150))
summary(NB.log)
# Eliminamos la marca del vehículo por ser muy poco significativa
NB.log <- update(NB.log, ~. -Veh_marca)
summary(NB.log)
# BUSCAMOS UN STEP PARA BUSCAR EL MEJOR MODELO
selec <- step(NB.log, direction='both', trace=1)
# Vemos un resumen del mejor modelo
summary(selec)
selec <- update(selec, ~. + I(Score2^3))
summary(selec)
VIF(selec)
test_data <- test_data %>% mutate(of_var = log(test_data$Expo/365.25))
test_data <- test_data %>% dplyr::select(-Expo)
p.NB.log <- predict.glm(object = selec, newdata = test_data , type="response" )
hist(p.NB.log)
ec <- function(prediccion, observado){
sum((prediccion-observado)^2)
}
ea <- function(prediccion, observado){
sum(abs(prediccion-observado))
}
ecm <- function(prediccion, observado){
sum((prediccion-observado)^2)/dim(test_data)[1]
}
ec(p.NB.log, test_data$N_totales)
ea(p.NB.log, test_data$N_totales)
ecm(p.NB.log, test_data$N_totales)
library(readxl)
data_test <- read_excel("./data/base_challenge.xlsx", sheet = "Test")
#View(data_test)
# Creamos las variables categóricas para el conjunto de test también. Solo lo hacemos para aquellas que están incluidas en el modelo.
# Primero replicamos los niveles de las variables categóricas usadas en el modelo durante el entrenamiento.
qntExpo <- quantbreaks(data$Expo, cortes=10,cuantiles=T)
qntCarne <- quantbreaks(data$Carne, cortes=4, cuantiles=T)
qntVeh_cdin <- quantbreaks(data$Veh_cdin, cortes=5, cuantiles=T)
mediana_expo <- median(data_test$Expo)
data_test$Expo[which(data_test$Expo==0)] <- mediana_expo
data_test <- data_test %>% mutate(Expo_Cat =cut(Expo, qntExpo), Carne_Cat=cut(Carne, qntCarne), Veh_cdin_Cat = cut(Veh_cdin, qntVeh_cdin), of_var = log(data_test$Expo/365.25))
p.NB.log.test <- predict.glm(object = selec, newdata = data_test , type="response" )
hist(p.NB.log.test)
# APARECEN NAs. COMO ESTAMOS HASTA LA POLLA, ASIGNAMOS LA MEDIANA
indices_na <- which(is.na(p.NB.log.test))
p.NB.log.test[indices_na] <- mean(p.NB.log.test, na.rm=T)
ec <- function(prediccion, observado){
sum((prediccion-observado)^2)
}
ea <- function(prediccion, observado){
sum(abs(prediccion-observado))
}
ecm <- function(prediccion, observado){
sum((prediccion-observado)^2)/dim(test_data)[1]
}
#ec(p.NB.log.test, test_data$N_totales)
#ea(p.NB.log.test, test_data$N_totales)
#ecm(p.NB.log.test, test_data$N_totales)
data_test <- data_test %>% mutate(Preds = p.NB.log.test)
save(data_test, file='./Predicciones.RData')
load('./Predicciones.RData')
discr<-discretize(x=data_test$Preds, method="frequency",labels = 1:10, breaks=10, ordered=TRUE)
data_test$intervalo_tarif<- discr
table(discr)
coste_medio <- (sum(data$C_rcmat_agregado) + sum(data$C_rccorp)) / (sum(data$N_rccorp)+sum(data$N_rcmat))
data_test <- data_test %>% mutate(ant_compni_cat = cut(ant_compnia, breaks=c(-1,2,4,9,50), labels=c(0,1,4,7)))
data_test <- data_test %>%  mutate(prima = coste_medio*(0.85+Preds) + 100 - 10*ant_compnia )
data_test$Prima <- data_test$prima
data_test$Exp_num_total_reclama <- data_test$Preds
data_test <- data_test %>% dplyr::select(-prima, -Preds)
resultados <- data_test %>% dplyr::select(poliza, Exp_num_total_reclama, Prima)
write.csv(resultados, file='./resultados.csv', row.names = F)
#vector de precios
correspondencia_precios<-c(100,150,200,250,300,500,800,1000,1800,2000)
names(correspondencia_precios)<-1:10
correspondencia_precios
for (i in 1:10){
data_test$Prima[data_test$intervalo_tarif==i]<-correspondencia_precios[i]
}
knitr::opts_chunk$set(echo = TRUE)
#cargamos las librerias
library(readxl)
library(ggplot2)
library(stats)
library(dplyr)
library(vcd)
library(glmnet)
library(stats)
library(caret)
library(pscl)
library(arules)
library(rsample)
library(MASS)
library(regclass)
library(randomForest)
library(reshape2)
data <- read_excel("./data/base_challenge.xlsx") %>%
filter( Veh_val > 0, Edad < 100) %>%  #quitamos las edades mayores a 100 y los valores de vehiculo negativos
dplyr::select(-C_postal, -C_rcmat_culpa, -C_rcmat_inoc) #vamos a modelar el número de reclamaciones, por ello eliminamos el coste de las mismas.
#algunas variables son factores
data$fpag <- factor(data$fpag, labels = c('Anual efectivo', 'Anual domiciliado',"semestral esfectivo", "semestral 1ºp efectivo", "semestral domiciliado"))
#data$C_postal <- factor(data$C_postal)
data$Veh_marca <- factor(data$Veh_marca)
data$Veh_cuso <- factor(data$Veh_cuso)#codificado en dos valores , 3 y 4. Pero no sabemos las etiquetas
data$Veh_comb <- factor(data$Veh_comb)
data$Cus_des <- factor(data$Cus_des)
data$b7_puertas <- factor(data$b7_puertas, ordered = TRUE)
#hacemos un resumen de los datos, a ver si todo esta correcto
str(data)
summary(data)
# Función que discretiza en cuantiles
quantbreaks <- function(predictor,cortes=10,cuantiles=F){
qnt <- quantile(predictor,probs=seq(0,1,1/cortes),include.lowest=T)
qnt[1] <- qnt[1]-0.01
qnt[cortes] <- qnt[cortes]+0.01
if(cuantiles){return(qnt)}                # devuelve cortes
else{return(cut(predictor,breaks=qnt))} # devuelve var discretizada
}
data2 <- data #vamos a modificar una copia de los datos originales
#segun la variable, definimos un numero diferente de cortes, pero tras unas pruebas, decidimos realizar 10 cortes en la mayoria.
data2 <- data2 %>% mutate(Edad_Cat = quantbreaks(data$Edad, cortes=10), Expo_Cat =quantbreaks(data$Expo, cortes=10),
Carne_Cat=quantbreaks(data$Carne, cortes=10), Veh_cdin_Cat = quantbreaks(data$Veh_cdin, cortes=5),
Veh_val_Cat=quantbreaks(data$Veh_val, cortes=10))
# Añadimos una columna con la suma de reclamaciones, pues será esta suma la que el modelo intentará predecir.
data3 <- data2 %>% dplyr::mutate(N_totales = N_rcmat + N_rccorp)
ggplot(data,aes(x = Score2))+geom_histogram()
rep <- filter(data, Veh_val < 100000)
hist(rep$Veh_val, breaks = 100) #Tenemos un valor solitario por debajo de 0, eliminar
hist(rep$Veh_peso, breaks = 100)
hist(data$veh_ant, breaks = 100)
#hist(data$b7_puertas, breaks = 100)
hist(data$b7_longitud, breaks = 100)
hist(data$ant_compnia, breaks = 100)
#hist(data$C_rcmat_culpa, breaks = 100)
#hist(data$C_rcmat_inoc, breaks = 100)
hist(data$veh_ant, breaks = 100)
data <- data %>% dplyr::mutate(N_totales=N_rcmat+N_rccorp)
data$N_totales <- factor(data$N_totales, ordered = TRUE)
for(i in colnames(data)){
if(class(data[[i]]) == "numeric"){
a <- ggplot(data = data, aes_string(x = "N_totales", y = i, fill = "N_totales"))+geom_violin()
print(a)
}
}
for(i in colnames(data3)){
if(class(data3[[i]]) == "character"){
mosaicplot( as.formula(paste0(" ~ ", i,"+N_totales")) ,data = data3)
}
}
for(i in colnames(data)){
if(class(data[[i]]) == "character"){
a <- xtabs(as.formula(paste0(" ~ ", i,"+N_rcmat")) ,data = data)
print(a)
}
}
# matriz de correlaciones para las variables anteriores
round(cor(data[,lapply(data,is.numeric) %>% unlist]),2)
cormat <- melt(round(cor(data[,lapply(data,is.numeric) %>% unlist]),2))
cormat %>% ggplot(aes(x=Var1,y=Var2,fill=value)) + geom_tile()+
scale_fill_gradient(low = "white", high = "red") +
geom_text(aes(label = value), color = "black", size = 2) +
coord_fixed() +
theme (axis.text.x = element_text(face="bold", colour="black",  angle = 90),
axis.text.y = element_text(face="bold", colour="black", hjust=0.5))
obs <- data3 %>% group_by(N_rcmat=N_rcmat) %>%
summarise(observados=round(n()/24995,3))
esperados <- round(dpois(0:4,mean(data3$N_rcmat)),3)
cbind(obs,esperados)
siniestros <- data3%>% group_by(N_rcmat)%>% summarise(observados=n()/dim(data3)[1])
siniestros <- siniestros%>% mutate(esperados=dpois(N_rcmat,lambda = mean(data3$N_rcmat)))
siniestros %>% ggplot()+geom_col(aes(x=N_rcmat,y=observados),color="blue",fill="blue",alpha=0.1)+
geom_col(aes(x=N_rcmat,y=esperados),color="red",fill="red",alpha=0.1)+ylab("Observado y Esperado")
fit.bn <- goodfit(data3$N_rcmat,type = "nbinomial", method="MinChisq")
# goodfit essentially computes the fitted values of a discrete distribution (either Poisson,
# binomial or negative binomial) to the count data given in x. If the parameters are not specified # they are estimated either by ML or Minimum Chi-squared.
negbinData <- data.frame(N_rcmat=0:4,
observados=fit.bn$observed/dim(data3)[1],
esperados=fit.bn$fitted/dim(data3)[1])
negbinData
negbinData%>% ggplot +
geom_col(aes(x=N_rcmat,y=observados),color="blue",fill="blue",alpha=0.1) +
geom_col(aes(x=N_rcmat,y=esperados),color="red",fill="red",alpha=0.05)+ylab("Observado y Esperado")
obs <- data3 %>% group_by(N_rccorp=N_rccorp) %>%
summarise(observados=round(n()/24995,3)) #24995 porque es el numero total de polizas consideradas
esperados <- round(dpois(0:2,mean(data3$N_rccorp)),3)
cbind(obs,esperados)
siniestros <- data3%>% group_by(N_rccorp)%>% summarise(observados=n()/dim(data3)[1])
siniestros <- siniestros%>% mutate(esperados=dpois(N_rccorp,lambda = mean(data3$N_rccorp)))
siniestros %>% ggplot()+geom_col(aes(x=N_rccorp,y=observados),color="blue",fill="blue",alpha=0.1)+
geom_col(aes(x=N_rccorp,y=esperados),color="red",fill="red",alpha=0.1)+ylab("Observado y Esperado")
obs <- data3 %>% group_by(N_totales=N_totales) %>%
summarise(observados=round(n()/24995,3))
esperados <- round(dpois(0:5,mean(data3$N_totales)),3)
cbind(obs,esperados)
siniestros <- data3%>% group_by(N_totales)%>% summarise(observados=n()/dim(data3)[1])
siniestros <- siniestros%>% mutate(esperados=dpois(N_totales,lambda = mean(data3$N_totales)))
siniestros %>% ggplot()+geom_col(aes(x=N_totales,y=observados),color="blue",fill="blue",alpha=0.1)+
geom_col(aes(x=N_totales,y=esperados),color="red",fill="red",alpha=0.1)+ylab("Observado y Esperado")
fit.bn <- goodfit(data3$N_totales,type = "nbinomial", method="MinChisq")
negbinData <- data.frame(N_totales=0:5,
observados=fit.bn$observed/dim(data3)[1],
esperados=fit.bn$fitted/dim(data3)[1])
negbinData%>% ggplot +
geom_col(aes(x=N_totales,y=observados),color="blue",fill="blue",alpha=0.1) +
geom_col(aes(x=N_totales,y=esperados),color="red",fill="red",alpha=0.05)+ylab("Observado y Esperado")
data3 <- data2 #copiamos los datos para modificarlos sin sobreescribir
#quitamos las variables numéricas que hemos categorizado y algunas mas que no consideramos que aporten al modelo
data4 <- data3 %>% dplyr::mutate(N_totales = N_rcmat + N_rccorp) %>%
dplyr::select(-poliza,
-Edad,-Carne,
-fini,-fvto,
-Veh_cdin,-Veh_val,
-Veh_peso,-Veh_cuso,-N_rcmat,-N_rccorp,-N_culpa,-N_inoc,-C_rcmat_agregado,-C_rccorp,
-Edad_Cat,
-veh_ant,-Cus_des,-clase_veh,-Tipo_veh)
#pruebas de discretizacion descartadas
#discretizamos en teniendo en cuenta la función objetivo
#library(OneR)
#var.num.disc <- optbin(formula=N_totales~Edad+Carne+Veh_cdin+Veh_val, data=data4, method = c("logreg"))
#var.num.disc <- var.num.disc %>% dplyr::select(-N_totales)
#data4[,names(var.num.disc)] <- var.num.disc
# Sustituimos los valores con Expo cero por la media de exposición
mediana_expo <- median(data4$Expo)
data4$Expo[which(data4$Expo==0)] <- mediana_expo
set.seed(1500)
particion <- initial_split(data4,prop = 2/3, strata = N_totales) # particion aleatoria
train_data <- training(particion)
test_data <- testing(particion)
# CREAMOS EL MODELO CON TODO Y HACEMOS UN STEP
# usamos una variable de offset que será el logaritmo del tiempo de exposición de la poliza
of_var <- log(train_data$Expo/365.25)
train_data <- train_data %>% dplyr::select(-Expo)
NB.log <- glm.nb(N_totales~.+offset(of_var) ,data=train_data,link="log",control=glm.control(maxit=150))
summary(NB.log)
# Eliminamos la marca del vehículo por ser muy poco significativa
NB.log <- update(NB.log, ~. -Veh_marca)
summary(NB.log)
# BUSCAMOS UN STEP PARA BUSCAR EL MEJOR MODELO
selec <- step(NB.log, direction='both', trace=1)
#resumen del mejor modelo encontrado
summary(selec)
selec <- update(selec, ~. + I(Score2^3))
summary(selec)
#vamos a comprobar si existe colinealidad entre variables predictoras mediante el test VIF
VIF(selec)
selec <- update(selec, ~. - I(Score2^3))
summary(selec)
#vamos a comprobar si existe colinealidad entre variables predictoras mediante el test VIF
VIF(selec)
test_data <- test_data %>% mutate(of_var = log(test_data$Expo/365.25))
test_data <- test_data %>% dplyr::select(-Expo)
p.NB.log <- predict.glm(object = selec, newdata = test_data , type="response" )
hist(p.NB.log)
ec <- function(prediccion, observado){
sum((prediccion-observado)^2)
}
ea <- function(prediccion, observado){
sum(abs(prediccion-observado))
}
ecm <- function(prediccion, observado){
sum((prediccion-observado)^2)/dim(test_data)[1]
}
resultados<-data.frame(error_cuadratico=ec(p.NB.log, test_data$N_totales),error_absoluto=ea(p.NB.log, test_data$N_totales)
, error_cuadratico_medio=ecm(p.NB.log, test_data$N_totales))
resultados
#Entrenamiento del modelo con todo el conjunto de datos.
of_var <- log(data4$Expo/365.25)
data4 <- data4 %>% dplyr::select(-Expo)
NB.log.final <- glm.nb(N_totales~b7_longitud+ant_compnia+Score2+Expo_Cat+Veh_cdin_Cat+offset(of_var)                       ,data=data4,link="log",control=glm.control(maxit=150))
summary(NB.log.final)
#cargamos los datos
library(readxl)
data_test <- read_excel("./data/base_challenge.xlsx", sheet = "Test")
#View(data_test)
# Creamos las variables categóricas para el conjunto de test también. Solo lo hacemos para aquellas que están incluidas en el modelo.
# Primero replicamos los niveles de las variables categóricas usadas en el modelo durante el entrenamiento.
qntExpo <- quantbreaks(data$Expo, cortes=10,cuantiles=T)
qntCarne <- quantbreaks(data$Carne, cortes=10, cuantiles=T)
qntVeh_cdin <- quantbreaks(data$Veh_cdin, cortes=5, cuantiles=T)
mediana_expo <- median(data_test$Expo)
data_test$Expo[which(data_test$Expo==0)] <- mediana_expo
data_test <- data_test %>% mutate(Expo_Cat =cut(Expo, qntExpo), Carne_Cat=cut(Carne, qntCarne), Veh_cdin_Cat = cut(Veh_cdin, qntVeh_cdin), of_var = log(data_test$Expo/365.25))
#usamos el modelo previamente entrenado
p.NB.log.test <- predict.glm(object = NB.log.final, newdata = data_test , type="response" )
hist(p.NB.log.test)
# APARECEN NAs. para lidiar con estos errores puntuales, asignamos la media. Esta decisión es arriesgada y no debería tenerse NUNCA
# como un procedimiento estándar. En este caso, evaluando manualmente las pólizas, consideramos que no hay mucho riesgo en tomar dicha decisión.
indices_na <- which(is.na(p.NB.log.test))
p.NB.log.test[indices_na] <- mean(p.NB.log.test, na.rm=T)
#anadimos la predicción al dataframe
data_test <- data_test %>% mutate(Preds = p.NB.log.test)
#y guardamos el dataframe para no tener que repetir todo el proceso por si cerramos el RStudio.
save(data_test, file='./Predicciones.RData')
#cargamos las predicciones en caso de que haga falta
load('./Predicciones.RData')
discr<-discretize(x=data_test$Preds, method="frequency",labels = 1:10, breaks=10, ordered=TRUE)
data_test$intervalo_tarif<- discr
table(discr)
#vector de precios
correspondencia_precios<-c(8,15,50,150,300,500,800,1000,1800,2000)
names(correspondencia_precios)<-1:10
correspondencia_precios
#añadimos los precios al dataset
for (i in 1:10){
data_test$Prima[data_test$intervalo_tarif==i]<-correspondencia_precios[i]
}
resultados <- data_test %>% dplyr::select(poliza, Preds, Prima)
write.csv(resultados, file='./resultados.csv', row.names = F)
resultados <- data_test %>% dplyr::select(poliza, Preds, Prima)
write.csv(resultados, file='./resultados_15mayo.csv', row.names = F)
which(is.na(resultados))
