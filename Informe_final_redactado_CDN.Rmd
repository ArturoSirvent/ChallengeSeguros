---
title: "Estimación de precios para una cartera de seguros"
author: "Francisco Olayo González, Ángel Guevara, Miguel Hortelano y Arturo Sirvent"
date: "14/5/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


Este informe resume los resultados finales de la modelización de una cartera de seguros, y su correspondiente tarificación para la asignatura Ciencia de Datos en Negocio, del Máster en Ciencia de Datos (UV) a 14 mayo de 2022.    
El trabajo se ha llevado a cabo en grupo. Autores: Arturo Sirvent, Francisco Olayo, Miguel Hortelano, Angel Guevara.  

# Carga y preprocesado de los datos  

```{r}
#cargamos las librerias
library(readxl)
library(ggplot2)
library(stats)
library(dplyr)
library(vcd)
library(glmnet)
library(stats)
library(caret)
library(pscl)
library(arules)
library(rsample)
library(MASS)
library(regclass)
library(randomForest)
library(reshape2)
```


Preparamos los datos, filtramos aquellos sin sentido o que juzgamos sospechosos, y pasamos algunas variables a factor.

```{r}

data <- read_excel("./data/base_challenge.xlsx") %>% 
  filter( Veh_val > 0, Edad < 100) %>%  #quitamos las edades mayores a 100 y los valores de vehiculo negativos
  dplyr::select(-C_postal, -C_rcmat_culpa, -C_rcmat_inoc) #vamos a modelar el número de reclamaciones, por ello eliminamos el coste de las mismas.
  

#algunas variables son factores
data$fpag <- factor(data$fpag, labels = c('Anual efectivo', 'Anual domiciliado',"semestral esfectivo", "semestral 1ºp efectivo", "semestral domiciliado"))
#data$C_postal <- factor(data$C_postal)
data$Veh_marca <- factor(data$Veh_marca)
data$Veh_cuso <- factor(data$Veh_cuso)#codificado en dos valores , 3 y 4. Pero no sabemos las etiquetas
data$Veh_comb <- factor(data$Veh_comb)
data$Cus_des <- factor(data$Cus_des)
data$b7_puertas <- factor(data$b7_puertas, ordered = TRUE)
```

El hecho de modelar el número de reclamaciones y no el coste de las mismas, es una decisión que se hace a priori, pero la elección contraria podría justificarse como válida igualmente.  

```{r}
#hacemos un resumen de los datos, a ver si todo esta correcto
str(data)
summary(data)
```

```{r}
# Función que discretiza en cuantiles
quantbreaks <- function(predictor,cortes=10,cuantiles=F){
  qnt <- quantile(predictor,probs=seq(0,1,1/cortes),include.lowest=T)
  qnt[1] <- qnt[1]-0.01
  qnt[cortes] <- qnt[cortes]+0.01 
  
  if(cuantiles){return(qnt)}                # devuelve cortes 
    else{return(cut(predictor,breaks=qnt))} # devuelve var discretizada
}
```

Hemos definido una función que nos ayuda a discretizar variables numéricas, y asi poder categorizarla y reducir la variabilidad del modelo. Así lo hacemos más robusto al ruido (pues literalmente estamos reduciendo los posibles inputs, definiendo de forma más certera los outputs).
Las divisiones de las discretizaciones se han hecho algunas atendiendo a como se distribuye la variable y otras simplemente atendiendo a lo que a nosotros nos parecía adecuado.
```{r}
data2 <- data #vamos a modificar una copia de los datos originales
#segun la variable, definimos un numero diferente de cortes, pero tras unas pruebas, decidimos realizar 10 cortes en la mayoria.
data2 <- data2 %>% mutate(Edad_Cat = quantbreaks(data$Edad, cortes=10), Expo_Cat =quantbreaks(data$Expo, cortes=10), 
                 Carne_Cat=quantbreaks(data$Carne, cortes=10), Veh_cdin_Cat = quantbreaks(data$Veh_cdin, cortes=5), 
                 Veh_val_Cat=quantbreaks(data$Veh_val, cortes=10))

# Añadimos una columna con la suma de reclamaciones, pues será esta suma la que el modelo intentará predecir.
data3 <- data2 %>% dplyr::mutate(N_totales = N_rcmat + N_rccorp)
```



# Análisis exploratorio de los datos

Empezamos revisando la distribución de algunas variables.

```{r}
ggplot(data,aes(x = Score2))+geom_histogram()

rep <- filter(data, Veh_val < 100000)

hist(rep$Veh_val, breaks = 100) #Tenemos un valor solitario por debajo de 0, eliminar
hist(rep$Veh_peso, breaks = 100)
hist(data$veh_ant, breaks = 100)
#hist(data$b7_puertas, breaks = 100)
hist(data$b7_longitud, breaks = 100)
hist(data$ant_compnia, breaks = 100)
#hist(data$C_rcmat_culpa, breaks = 100)
#hist(data$C_rcmat_inoc, breaks = 100)
hist(data$veh_ant, breaks = 100)
```

Podemos observar como se distribuyen las variables mediante los histogramas. Vemos como por ejemplo la variable `Veh_peso` está sobre todo entre 1000 y 1500 o como la variable `Veh_val` se encuentra en torno al valor 20000, habiendo algunos datos que se separan un poco del resto llegando incluso a 100000. 

Los histogramas son muy útiles también para detectar outliers que no siguen el mismo comportamiento que la mayoría de los datos.


Distribución de variables numéricas según el número de accindentes totales.
```{r}
data <- data %>% dplyr::mutate(N_totales=N_rcmat+N_rccorp)
data$N_totales <- factor(data$N_totales, ordered = TRUE)

for(i in colnames(data)){
  if(class(data[[i]]) == "numeric"){
    a <- ggplot(data = data, aes_string(x = "N_totales", y = i, fill = "N_totales"))+geom_violin()
    print(a)
  }
  
}
```

Los gráficos de violín también son muy útiles para ver como se distribuyen las variables y para ver la existencia de grupos con respecto a una variable de estudio. En particular, hemos elegido comparar cada variable con el número de accidentes totales, que será la variable que queramos modelizar más adelante.

En general, vemos como el comportamiento de la mayoría de las variables no cambia conforme aumenta el número de accidentes totales. 

Además, tampoco se ve muy claro en estos casos la existencia de grupos muy definidos en las variables. Si bien es cierto, que en algunas se puede intuir un poco y darnos alguna orientación a la hora de discretizar variables si fuera necesario. Por ejemplo, vemos como en la variable `Veh_cdin` pueden existir en torno a 5-6 grupos cuando el número total de accidentes es cero. Otro ejemplo claro es la variable `veh_ant`, dónde vemos la existencia de grupos muy definidos en todos los casos de `N_totales`. Esto se debe a que es una variable numérica discreta. 

En cuánto a las variables `N_rcmat` y `N_rccorp`, evidentemente vemos como su distribución va tomando valores más altos conforme aumenta el número de accidentes totales (`N_totales`).


Mosaico de variables categóricas según número de accidentes
```{r}

for(i in colnames(data3)){
  if(class(data3[[i]]) == "character"){
    mosaicplot( as.formula(paste0(" ~ ", i,"+N_totales")) ,data = data3)
  }
  
}
```

En los gráficos de mosaico podemos observar por ejemplo como la mayoría de los clientes usan un TURISMO como vehículo o como la mayoría de esos vehículos son tipo berlina. Esta información puede ser interesante de cara al modelo y puede servir para prestar más atención a ciertos detalles.

Tablas de contingencia de variables categóricas según número de accidentes. Es la misma información que obteníamos con el gráfico de mosaico, pero en forma de tabla. Quizás en las tablas se ven más claro las diferentes categorías de cada variable.
```{r}
for(i in colnames(data)){
  if(class(data[[i]]) == "character"){
    a <- xtabs(as.formula(paste0(" ~ ", i,"+N_rcmat")) ,data = data)
    print(a)
  }
  
}
```



Ahora vemos si existen correlaciones entre las variables. Esto será interesante de cara al modelo, para no incluir dos variables que contengan la misma información y que por tanto sean redundantes.

```{r}
# matriz de correlaciones para las variables anteriores
round(cor(data[,lapply(data,is.numeric) %>% unlist]),2)
```

También podemos ver el mismo resultado de manera gráfica.
```{r}
cormat <- melt(round(cor(data[,lapply(data,is.numeric) %>% unlist]),2))

cormat %>% ggplot(aes(x=Var1,y=Var2,fill=value)) + geom_tile()+  
  scale_fill_gradient(low = "white", high = "red") +
  geom_text(aes(label = value), color = "black", size = 2) +
  coord_fixed() + 
  theme (axis.text.x = element_text(face="bold", colour="black",  angle = 90),
           axis.text.y = element_text(face="bold", colour="black", hjust=0.5))
```

Tras hacer las correlaciones cruzadas entre todas las variables, observamos como hay algunas variables que están altamente correladas positivamente. Por ejemplo, `Carne` y `Edad` tienen una correlación de 0.91, cosa que es bastante y que tiene sentido, porque a mayor edad, más años de carnet en principio.

Tanto la primera columna como la última fila no las tenemos en cuenta ya que hacen referencia al número de poliza, que al fin y al cabo no es más que un número que asignamos de forma aleatoria a los clientes. 

La matriz de correlaciones da pie a eliminar algunas variables de cara al entrenamiento del modelo. Las decisiones que hemos tomado son las siguientes: 

* Existe una alta correlación entre `Edad` y `Carne`. Nos quedamos con la variable `Carne` ya que nos parece más informativa en algunos casos. Por ejemplo, se puede dar el caso de una persona mayor que tenga pocos años de carnet (aunque no es lo común). Sin embargo, si tienes muchos años de carnet, por lo general vas a ser una persona de mayor edad.


* También hay mucha correlación entre las variables `Veh_cdin` y `Veh_peso`. En este caso decidimos quedarnos con las potencia por encima del peso. 


* De igual forma, `Veh_peso` y `b7_longitud` están altamente correladas. Nos quedaremos con la longitud del coche en este caso ya que antes hemos desechado el peso.

* Por último, hay una gran correlación entre la antigüedad en la compañía (`ant_compnia`) y la antigüedad del vehículo (`veh_ant`). En este caso, decidimos quedarnos con la `ant_compnia` ya que nos parece más informativa.

Ahora que tenemos los datos preparados, podemos proceder a definir el modelo que realizará la estimación. 

# Elección del modelo


## Distribución de la variable respuesta  

Observamos que si modelamos la variable de reclamacions materiales `N_rcmat` con una distribución de poisson, no existe exceso de ceros en la variable.
```{r}
obs <- data3 %>% group_by(N_rcmat=N_rcmat) %>% 
  summarise(observados=round(n()/24995,3))
esperados <- round(dpois(0:4,mean(data3$N_rcmat)),3)
cbind(obs,esperados)
```


Representado los datos mediante barras con la altura de la frecuencia esperada y observada, vemos que puede ser una poisson, pues superponen ambas.
```{r}
siniestros <- data3%>% group_by(N_rcmat)%>% summarise(observados=n()/dim(data3)[1])
siniestros <- siniestros%>% mutate(esperados=dpois(N_rcmat,lambda = mean(data3$N_rcmat)))

siniestros %>% ggplot()+geom_col(aes(x=N_rcmat,y=observados),color="blue",fill="blue",alpha=0.1)+
  geom_col(aes(x=N_rcmat,y=esperados),color="red",fill="red",alpha=0.1)+ylab("Observado y Esperado")
```

Sin embargo, también podría ser una binomial negativa. De hecho, visualmente parece que ajusta un poco mejor con la binomial negativa que con la poisson.   
```{r}
fit.bn <- goodfit(data3$N_rcmat,type = "nbinomial", method="MinChisq")

# goodfit essentially computes the fitted values of a discrete distribution (either Poisson, 
# binomial or negative binomial) to the count data given in x. If the parameters are not specified # they are estimated either by ML or Minimum Chi-squared.

negbinData <- data.frame(N_rcmat=0:4,
                         observados=fit.bn$observed/dim(data3)[1], 
                         esperados=fit.bn$fitted/dim(data3)[1])
negbinData

negbinData%>% ggplot +
  geom_col(aes(x=N_rcmat,y=observados),color="blue",fill="blue",alpha=0.1) +
  geom_col(aes(x=N_rcmat,y=esperados),color="red",fill="red",alpha=0.05)+ylab("Observado y Esperado")
```

Con la binomial negativa se produce un ajuste más fino a las frecuencias esperadas.  

Al estudiar la distribución de la variable `N_rccorp`, vemos un buen ajuste a una distribución de poisson.  
```{r}
obs <- data3 %>% group_by(N_rccorp=N_rccorp) %>% 
  summarise(observados=round(n()/24995,3)) #24995 porque es el numero total de polizas consideradas 
esperados <- round(dpois(0:2,mean(data3$N_rccorp)),3)
cbind(obs,esperados)
```
Y de forma gráfica se ve claro la superposición entre observado y esperado.  
```{r}
siniestros <- data3%>% group_by(N_rccorp)%>% summarise(observados=n()/dim(data3)[1])
siniestros <- siniestros%>% mutate(esperados=dpois(N_rccorp,lambda = mean(data3$N_rccorp)))

siniestros %>% ggplot()+geom_col(aes(x=N_rccorp,y=observados),color="blue",fill="blue",alpha=0.1)+
  geom_col(aes(x=N_rccorp,y=esperados),color="red",fill="red",alpha=0.1)+ylab("Observado y Esperado")
```

Sin embargo, hemos decidido que vamos a predecir la variable respuesta `N_totales` que son las reclamaciones totales, independientemente de su naturaleza.  
Comprobamos que sí existe un exceso de ceros en la variable suma `N_rcmat + N_rccorp`.  
```{r}
obs <- data3 %>% group_by(N_totales=N_totales) %>% 
  summarise(observados=round(n()/24995,3))
esperados <- round(dpois(0:5,mean(data3$N_totales)),3)
cbind(obs,esperados)

siniestros <- data3%>% group_by(N_totales)%>% summarise(observados=n()/dim(data3)[1])
siniestros <- siniestros%>% mutate(esperados=dpois(N_totales,lambda = mean(data3$N_totales)))

siniestros %>% ggplot()+geom_col(aes(x=N_totales,y=observados),color="blue",fill="blue",alpha=0.1)+
  geom_col(aes(x=N_totales,y=esperados),color="red",fill="red",alpha=0.1)+ylab("Observado y Esperado")
```

Debido a la posibilidad de un exceso de ceros en la variable respueta `N_totales`, podría hacer falta usar un modelo para corregir dicho exceso. 

Hemos comenzado planteando un modelo de Poisson, pero también sería conveniente probar alguno con inflación de ceros como la binomial negativa.

Otra opción podría ser usar otro tipo de modelos como modelos de mezcla (hurdle: modeliza los ceros con una bernouilli y el resto de valores con una poisson truncada. Útil cuando hay muchos o pocos ceros), árboles de decisión, bagging,... pero lo plantearemos en el futuro si las opciones más sencillas no resultan.


Comparamos con la distribución de binomial negativa.
```{r}
fit.bn <- goodfit(data3$N_totales,type = "nbinomial", method="MinChisq")
negbinData <- data.frame(N_totales=0:5,
                         observados=fit.bn$observed/dim(data3)[1], 
                         esperados=fit.bn$fitted/dim(data3)[1])

negbinData%>% ggplot +
  geom_col(aes(x=N_totales,y=observados),color="blue",fill="blue",alpha=0.1) +
  geom_col(aes(x=N_totales,y=esperados),color="red",fill="red",alpha=0.05)+ylab("Observado y Esperado")
```
Vemos que cuando usamos la suma de reclamaciones, la distribución se ajusta muy bien a una binomial negativa. Esta distribución es capaz de tener en cuenta ese ligero exceso de ceros, y por ello usaremos para modelizar la binomial negativa.


Para cada uno de los posibles modelos que podamos elegir, habrá que decidir si le metemos la variable de offset o no. Otro de los aspectos que tenemos que decidir, es si usar regularización o no. Esto puede ser útil para evitar el sobreajuste si nos encontramos en dicho caso (modelos LASSO, RIDGE, ELASTICNET).

Se probarán diferentes cosas, y al final nos quedaremos con el que mejor resultados nos dé.

## Construcción del modelo  

Probamos a modelar el número total de reclamaciones con una binomial negativa, ya que hemos visto que sigue prácticamente dicha distribución.

```{r}
data3 <- data2 #copiamos los datos para modificarlos sin sobreescribir

#quitamos las variables numéricas que hemos categorizado y algunas mas que no consideramos que aporten al modelo
data4 <- data3 %>% dplyr::mutate(N_totales = N_rcmat + N_rccorp) %>%
                  dplyr::select(-poliza,
                              -Edad,-Carne,
                              -fini,-fvto,
                              -Veh_cdin,-Veh_val,
                              -Veh_peso,-Veh_cuso,-N_rcmat,-N_rccorp,-N_culpa,-N_inoc,-C_rcmat_agregado,-C_rccorp,
                              -Edad_Cat,
                              -veh_ant,-Cus_des,-clase_veh,-Tipo_veh)


#pruebas de discretizacion descartadas
#discretizamos en teniendo en cuenta la función objetivo
#library(OneR)
#var.num.disc <- optbin(formula=N_totales~Edad+Carne+Veh_cdin+Veh_val, data=data4, method = c("logreg"))
#var.num.disc <- var.num.disc %>% dplyr::select(-N_totales)
#data4[,names(var.num.disc)] <- var.num.disc
```


Vamos a añadir un offset al modelo, pues la exposición de las pólizas no es constante, y cada una tiene una duración diferente, esto se debe tener en cuenta si o si, porque una póliza que dure el doble, tendrá mayor probabilidad de accidente.  
Además hay algunas pólizas con una exposición / duración de 0, esto dará error en el modelo pues se toma el logaritmo del offset. Para remediarlo imputamos dichas pólizas con la mediana de exposición del resto.
```{r}
# Sustituimos los valores con Expo cero por la media de exposición 
mediana_expo <- median(data4$Expo)
data4$Expo[which(data4$Expo==0)] <- mediana_expo
```

Hacemos una separación de los datos en train y test.
```{r}
set.seed(1500)
particion <- initial_split(data4,prop = 2/3, strata = N_totales) # particion aleatoria 
train_data <- training(particion)
test_data <- testing(particion)
```




```{r}
# CREAMOS EL MODELO CON TODO Y HACEMOS UN STEP

# usamos una variable de offset que será el logaritmo del tiempo de exposición de la poliza
of_var <- log(train_data$Expo/365.25)
train_data <- train_data %>% dplyr::select(-Expo)

NB.log <- glm.nb(N_totales~.+offset(of_var) ,data=train_data,link="log",control=glm.control(maxit=150))
summary(NB.log)

# Eliminamos la marca del vehículo por ser muy poco significativa 
NB.log <- update(NB.log, ~. -Veh_marca)
summary(NB.log)
```

Hemos creado un modelo con las variables que creíamos más importantes, y lo hemos corregido según la relevancia que el modelo le asigna a cada una, pero este proceso se puede complicar mucho. Por eso vamos a recurrir a la automatización que nos proporciona R mediante la función `step()`, que prueba diferentes combinaciones de variables.   

```{r}
# BUSCAMOS UN STEP PARA BUSCAR EL MEJOR MODELO 

selec <- step(NB.log, direction='both', trace=1)

```

```{r}
#resumen del mejor modelo encontrado
summary(selec)
```



Tras muchas pruebas, nos damos cuenta que añadir el `Score2` como término cúbico mejora el modelo.

```{r}
selec <- update(selec, ~. + I(Score2^3))
summary(selec)
```

Es bien sabido que la colinealidad entre variables predictoras en los modelos lineales, puede derivar en inestabilidades de los resultados, pues se supone que dichas variables aportan efectos independientes a la variable respuesta. 
```{r}
#vamos a comprobar si existe colinealidad entre variables predictoras mediante el test VIF

VIF(selec)

```
Con el VIF, podemos ver si existe conlinealdidad entre nuestras variables. En este caso, vemos como existe una gran colinealidad entre las variables `Score2` y `Score2` al cubo. Por tanto, no podemos incluirla en el modelo.


```{r}
selec <- update(selec, ~. - I(Score2^3))
summary(selec)
```

Volvemos a comprobar la colinealidad. 

```{r}
#vamos a comprobar si existe colinealidad entre variables predictoras mediante el test VIF

VIF(selec)

```

Ahora mejor, ya no tenemos colinealidad entre nuestras variables.

Ahora que tenemos un modelo, vamos a realizar una predicción sobre el conjunto de test.  
```{r}
test_data <- test_data %>% mutate(of_var = log(test_data$Expo/365.25))
test_data <- test_data %>% dplyr::select(-Expo)

p.NB.log <- predict.glm(object = selec, newdata = test_data , type="response" )
hist(p.NB.log)

ec <- function(prediccion, observado){
  sum((prediccion-observado)^2)
}

ea <- function(prediccion, observado){
  sum(abs(prediccion-observado))
}

ecm <- function(prediccion, observado){
  sum((prediccion-observado)^2)/dim(test_data)[1]
}

resultados<-data.frame(error_cuadratico=ec(p.NB.log, test_data$N_totales),error_absoluto=ea(p.NB.log, test_data$N_totales)
                       , error_cuadratico_medio=ecm(p.NB.log, test_data$N_totales))
resultados
```

Con esto podemos obtener unos errores y valores que nos han servido para comprar entre distintos modelos. A menor error, más precisa la predicción. 


Nosotros hemos separado un conjunto como test, pero las predicciones finales las debemos realizar sobre un conjunto de datos del que NO tenemos la variables respuesta, esto sería igual que el proceso de 'desplegar' un modelo, donde se pone en producción para datos con los que ya no podemos confirmar que tal lo ha hecho, pues no tenemos las respuestas (`y_true`). Para hacer la predicción final vamos a entrenar el modelo con todos los datos disponibles y no solo 2/3 como hemos hecho antes.

Para ello, recordemos que en la variable `data4` tenemos la versión de los datos con el preprocesado ya hecho. Por tanto, esa será la base de datos que tenemos que utilizar para entrenar el modelo final con las mejores variables para nosotros.

```{r}
#Entrenamiento del modelo con todo el conjunto de datos.

of_var <- log(data4$Expo/365.25)
data4 <- data4 %>% dplyr::select(-Expo)

NB.log.final <- glm.nb(N_totales~b7_longitud+ant_compnia+Score2+Expo_Cat+Veh_cdin_Cat+offset(of_var)                       ,data=data4,link="log",control=glm.control(maxit=150))
summary(NB.log.final)

```

Vemos que únicamente hay dos categorías de la variable `Veh_cdin_Cat` que no son significativas, pero tampoco se quedan muy lejos de serlo. Por tanto, las dejamos en el modelo.

# Predicciones finales

Cargamos los datos a predecir y obtenemos la estimación de siniestros para las pólizas.

```{r}
#cargamos los datos
library(readxl)
data_test <- read_excel("./data/base_challenge.xlsx", sheet = "Test")
#View(data_test)
```

```{r}
# Creamos las variables categóricas para el conjunto de test también. Solo lo hacemos para aquellas que están incluidas en el modelo.

# Primero replicamos los niveles de las variables categóricas usadas en el modelo durante el entrenamiento. 

qntExpo <- quantbreaks(data$Expo, cortes=10,cuantiles=T)
qntCarne <- quantbreaks(data$Carne, cortes=10, cuantiles=T) 
qntVeh_cdin <- quantbreaks(data$Veh_cdin, cortes=5, cuantiles=T)

mediana_expo <- median(data_test$Expo)
data_test$Expo[which(data_test$Expo==0)] <- mediana_expo

data_test <- data_test %>% mutate(Expo_Cat =cut(Expo, qntExpo), Carne_Cat=cut(Carne, qntCarne), Veh_cdin_Cat = cut(Veh_cdin, qntVeh_cdin), of_var = log(data_test$Expo/365.25))
```


```{r}
#usamos el modelo previamente entrenado
p.NB.log.test <- predict.glm(object = NB.log.final, newdata = data_test , type="response" )
hist(p.NB.log.test)

# APARECEN NAs. para lidiar con estos errores puntuales, asignamos la media. Esta decisión es arriesgada y no debería tenerse NUNCA
# como un procedimiento estándar. En este caso, evaluando manualmente las pólizas, consideramos que no hay mucho riesgo en tomar dicha decisión.

indices_na <- which(is.na(p.NB.log.test))
p.NB.log.test[indices_na] <- mean(p.NB.log.test, na.rm=T)
```


```{r}
#anadimos la predicción al dataframe
data_test <- data_test %>% mutate(Preds = p.NB.log.test)
```

```{r}
#y guardamos el dataframe para no tener que repetir todo el proceso por si cerramos el RStudio.
save(data_test, file='./Predicciones.RData')
```

# Tarificación

Ahora solo nos queda realizar la tarificación de precios. Este proceso lo hemos ido cambiando con el tiempo, conforme se competía con los otros equipos respecto a obtener los mejores precios para las mejores pólizas.  

Nuestra estrategia ha consistido en no arriesgar con pólizas posiblemente problemáticas, y asegurar las más seguras con precios muy bajos.  
Los precios los hemos puesto manualmente a tramos de igual cantidad de pólizas.  

```{r}
#cargamos las predicciones en caso de que haga falta
load('./Predicciones.RData')
```

Y ordenamos en 10 tramos a las diferentes pólizas según su riesgo.  
```{r}
discr<-discretize(x=data_test$Preds, method="frequency",labels = 1:10, breaks=10, ordered=TRUE)
data_test$intervalo_tarif<- discr
table(discr)

```

Hemos hecho segmentos y ahora le asignamos unos precios a ojo a estos segmentos, siendo los de los últimos segmentos, claramente prohibitivos.  

```{r}
#vector de precios
correspondencia_precios<-c(8,15,50,150,300,500,800,1000,1800,2000)
names(correspondencia_precios)<-1:10
correspondencia_precios
```


```{r}
#añadimos los precios al dataset
for (i in 1:10){
  data_test$Prima[data_test$intervalo_tarif==i]<-correspondencia_precios[i]
}
```

Y finalmente guardamos los resultados. 
```{r}
resultados <- data_test %>% dplyr::select(poliza, Preds, Prima)

write.csv(resultados, file='./resultados.csv', row.names = F)
```

